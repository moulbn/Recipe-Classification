{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54c91b3",
   "metadata": {},
   "source": [
    "# Exam Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f181068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d985f367",
   "metadata": {},
   "source": [
    "## I. Reading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16808ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT = \"0123456789!#$%&()''*+-/.:;?@[]{}|^_`~<>=\\\"\" # all punctuation we discard\n",
    "TABLE = str.maketrans(PUNCT, \" \" * len(PUNCT)) # replace punctuation by space\n",
    "\n",
    "def read_document(filename):\n",
    "    f = open(filename, encoding=\"utf-8\") # specify encoding to avoid unreadable documents\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    text = text.lower() # all words to lowercase\n",
    "    text = text.translate(TABLE)\n",
    "    words = text.split() # separate the document into list of words\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8c70f",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f60f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['warm', 'cherry', 'bread', 'pudding', 'ingredients', 'inch', 'thick', 'slices', 'challah', 'or', 'other', 'egg', 'bread', 'about', 'ounces', 'cups', 'whipping', 'cream', 'cups', 'whole', 'milk', 'cups', 'sugar', 'large', 'eggs', 'cup', 'dark', 'rum', 'tablespoons', 'vanilla', 'extract', 'cups', 'dried', 'tart', 'cherries', 'purchased', 'caramel', 'sauce,', 'heated', 'preparation', 'preheat', 'oven', 'to', '°f', 'using', 'inch', 'diameter', 'round', 'cookie', 'cutter,', 'cut', 'round', 'from', 'each', 'bread', 'slice', 'and', 'arrange', 'on', 'baking', 'sheet', 'toast', 'bread', 'rounds', 'in', 'oven', 'until', 'golden', 'brown,', 'about', 'minutes', 'reduce', 'oven', 'temperature', 'to', '°f', 'bring', 'cream,', 'milk,', 'and', 'sugar', 'to', 'simmer', 'in', 'heavy', 'large', 'saucepan,', 'stirring', 'to', 'dissolve', 'sugar', 'whisk', 'eggs', 'in', 'large', 'bowl', 'to', 'blend', 'gradually', 'whisk', 'hot', 'cream', 'mixture', 'into', 'eggs', 'whisk', 'in', 'rum', 'and', 'vanilla', 'butter', 'eight', 'cup', 'soufflé', 'dishes', 'arrange', 'on', 'rimmed', 'baking', 'sheet', 'place', 'bread', 'round', 'in', 'bottom', 'of', 'each', 'dish', 'top', 'each', 'with', 'heaping', 'tablespoon', 'dried', 'cherries', 'top', 'cherries', 'in', 'each', 'dish', 'with', 'another', 'bread', 'round', 'top', 'each', 'with', 'another', 'heaping', 'tablespoon', 'dried', 'cherries', 'top', 'cherries', 'in', 'each', 'dish', 'with', 'third', 'bread', 'slice', 'divide', 'custard', 'among', 'dishes,', 'using', 'about', 'cup', 'for', 'each', 'let', 'stand', 'minutes,', 'pressing', 'down', 'on', 'bread', 'occasionally', 'can', 'be', 'made', 'hours', 'ahead', 'cover', 'and', 'refrigerate', 'bake', 'bread', 'puddings', 'until', 'tops', 'are', 'puffed', 'and', 'brown,', 'about', 'minutes', 'remove', 'from', 'oven', 'cool', 'minutes', 'serve', 'warm', 'with', 'caramel', 'sauce']\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(\"recipes/test\"):\n",
    "    path = \"recipes/test/\" + f\n",
    "    words = read_document(path)\n",
    "    \n",
    "    print(words)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4bee4",
   "metadata": {},
   "source": [
    "### Stemming and Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e69565",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"recipes/\")\n",
    "sw_path = \"recipes/stopwords.txt\"\n",
    "stopwords = read_document(sw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a16d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmed_document(filename):\n",
    "    words = read_document(filename)\n",
    "    \n",
    "    # Removing Stopwords\n",
    "    words_without_sw = [i for i in words if i not in stopwords]\n",
    "    \n",
    "    #Stemming\n",
    "    ps = PorterStemmer()\n",
    "    stemmed = [ps.stem(word) for word in words_without_sw]\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c34ef",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdd7dec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['warm', 'cherri', 'bread', 'pud', 'ingredi', 'inch', 'slice', 'challah', 'egg', 'bread', 'ounc', 'cup', 'whip', 'cream', 'cup', 'milk', 'cup', 'sugar', 'larg', 'egg', 'cup', 'dark', 'rum', 'tablespoon', 'vanilla', 'extract', 'cup', 'dri', 'tart', 'cherri', 'purchas', 'caramel', 'sauce,', 'heat', 'prepar', 'preheat', 'oven', '°f', 'use', 'inch', 'diamet', 'round', 'cooki', 'cutter,', 'cut', 'round', 'bread', 'slice', 'arrang', 'bake', 'sheet', 'toast', 'bread', 'round', 'oven', 'golden', 'brown,', 'minut', 'reduc', 'oven', 'temperatur', '°f', 'bring', 'cream,', 'milk,', 'sugar', 'simmer', 'heavi', 'larg', 'saucepan,', 'stir', 'dissolv', 'sugar', 'whisk', 'egg', 'larg', 'bowl', 'blend', 'gradual', 'whisk', 'hot', 'cream', 'mixtur', 'egg', 'whisk', 'rum', 'vanilla', 'butter', 'cup', 'soufflé', 'dish', 'arrang', 'rim', 'bake', 'sheet', 'place', 'bread', 'round', 'dish', 'heap', 'tablespoon', 'dri', 'cherri', 'cherri', 'dish', 'bread', 'round', 'heap', 'tablespoon', 'dri', 'cherri', 'cherri', 'dish', 'bread', 'slice', 'divid', 'custard', 'dishes,', 'use', 'cup', 'let', 'stand', 'minutes,', 'press', 'bread', 'occasion', 'hour', 'ahead', 'cover', 'refriger', 'bake', 'bread', 'pud', 'top', 'puf', 'brown,', 'minut', 'remov', 'oven', 'cool', 'minut', 'serv', 'warm', 'caramel', 'sauc']\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(\"recipes/test\"):\n",
    "    path = \"recipes/test/\" + f\n",
    "    words = stemmed_document(path)\n",
    "    \n",
    "    print(words)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8602b4a",
   "metadata": {},
   "source": [
    "## II. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b54c0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = collections.Counter()\n",
    "\n",
    "for f in os.listdir(\"recipes/train\"):\n",
    "    path = \"recipes/train/\" + f\n",
    "    words = stemmed_document(path)\n",
    "    \n",
    "    vocabulary.update(words) # count words and add to vocabulary\n",
    "    \n",
    "# Save vocabulary in \"vocabulary.txt\" file\n",
    "f = open(\"vocabulary.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for word, count in vocabulary.most_common(1000): # 1000 most common words\n",
    "    print(word, file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f798c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocabulary(filename):\n",
    "    f = open(filename, encoding=\"utf-8\")\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    words = text.split()\n",
    "    \n",
    "    # Create index for each word\n",
    "    vocab = {}\n",
    "    index = 0\n",
    "    for word in words:\n",
    "        vocab[word] = index\n",
    "        index += 1\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab009bf",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a6368d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cup', 0), ('tablespoon', 1), ('minut', 2), ('teaspoon', 3), ('add', 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = load_vocabulary(\"vocabulary.txt\")\n",
    "list(vocabulary.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd92fbb",
   "metadata": {},
   "source": [
    "## III. Bag of Words Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e690beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(filename, vocab):\n",
    "    words = stemmed_document(filename)\n",
    "    \n",
    "    # Bag of Words\n",
    "    bow = np.zeros(len(vocab))\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            index = vocab[word]\n",
    "            bow[index] += 1\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827af33b",
   "metadata": {},
   "source": [
    "#### Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e135b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "for f in os.listdir(\"recipes/train\"):\n",
    "    if \"american\" in f:\n",
    "        final_path = \"recipes/train/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(0)\n",
    "\n",
    "for f in os.listdir(\"recipes/train\"):\n",
    "    if \"asian\" in f:\n",
    "        final_path = \"recipes/train/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(1)\n",
    "\n",
    "for f in os.listdir(\"recipes/train\"):\n",
    "    if \"french\" in f:\n",
    "        final_path = \"recipes/train/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(2)\n",
    "\n",
    "for f in os.listdir(\"recipes/train\"):\n",
    "    if \"indian\" in f:\n",
    "        final_path = \"recipes/train/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(3)\n",
    "\n",
    "for f in os.listdir(\"recipes/train\"):\n",
    "    if \"italian\" in f:\n",
    "        final_path = \"recipes/train/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(4)\n",
    "\n",
    "for f in os.listdir(\"recipes/train\"):\n",
    "    if \"jewish\" in f:\n",
    "        final_path = \"recipes/train/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(5)\n",
    "\n",
    "for f in os.listdir(\"recipes/train\"):\n",
    "    if \"mexican\" in f:\n",
    "        final_path = \"recipes/train/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(6)\n",
    "\n",
    "for f in os.listdir(\"recipes/train\"):\n",
    "    if \"middle_eastern\" in f:\n",
    "        final_path = \"recipes/train/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(7)\n",
    "\n",
    "\n",
    "X = np.stack(documents)\n",
    "Y = np.array(labels)\n",
    "\n",
    "data = np.concatenate([X, Y[:, None]], 1)\n",
    "\n",
    "np.savetxt(\"train_bow.txt.gz\", data) # Save into a file / .gz compresses the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197773ef",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be6fba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "for f in os.listdir(\"recipes/test\"):\n",
    "    if \"american\" in f:\n",
    "        final_path = \"recipes/test/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(0)\n",
    "\n",
    "for f in os.listdir(\"recipes/test\"):\n",
    "    if \"asian\" in f:\n",
    "        final_path = \"recipes/test/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(1)\n",
    "\n",
    "for f in os.listdir(\"recipes/test\"):\n",
    "    if \"french\" in f:\n",
    "        final_path = \"recipes/test/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(2)\n",
    "\n",
    "for f in os.listdir(\"recipes/test\"):\n",
    "    if \"indian\" in f:\n",
    "        final_path = \"recipes/test/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(3)\n",
    "\n",
    "for f in os.listdir(\"recipes/test\"):\n",
    "    if \"italian\" in f:\n",
    "        final_path = \"recipes/test/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(4)\n",
    "\n",
    "for f in os.listdir(\"recipes/test\"):\n",
    "    if \"jewish\" in f:\n",
    "        final_path = \"recipes/test/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(5)\n",
    "\n",
    "for f in os.listdir(\"recipes/test\"):\n",
    "    if \"mexican\" in f:\n",
    "        final_path = \"recipes/test/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(6)\n",
    "\n",
    "for f in os.listdir(\"recipes/test\"):\n",
    "    if \"middle_eastern\" in f:\n",
    "        final_path = \"recipes/test/\" + f\n",
    "        bow = bag_of_words(final_path, vocabulary)\n",
    "        documents.append(bow)\n",
    "        labels.append(7)\n",
    "\n",
    "\n",
    "X = np.stack(documents)\n",
    "Y = np.array(labels)\n",
    "\n",
    "data = np.concatenate([X, Y[:, None]], 1)\n",
    "\n",
    "np.savetxt(\"test_bow.txt.gz\", data) # Save into a file / .gz compresses the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe9d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
